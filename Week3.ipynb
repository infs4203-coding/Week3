{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bdfca1",
   "metadata": {},
   "source": [
    "The main purpose of this document is to introduce how to apply two classifiers, **decision tree** and **random forest**, implemented by [scikit-learn](https://scikit-learn.org/stable/). We will use the modified Iris dataset introduced in Week 2, and assume we have already completed the imputation, so there are no null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79fdd60",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641ff27",
   "metadata": {},
   "source": [
    "We first import the packages that will be used in this document.\n",
    "\n",
    "1. [Pandas](https://pandas.pydata.org/): Pandas is an open-source Python library widely used for data manipulation, analysis, and cleaning tasks. The central data structure in Pandas is the [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) which provides methods to facilitate the preliminary examination of essential properties, statistical summaries, and a select number of rows for a cursory exploration of the data.\n",
    "\n",
    "2. [Numpy](https://numpy.org/): Numpy is a powerful Python library for numerical and array-based computing. It provides support for large, multi-dimensional arrays and matrices, along with a wide range of mathematical functions to operate on these arrays efficiently. \n",
    "\n",
    "3. [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html): OneHotEncoder is a package in the scikit-learn library (sklearn) used for one-hot encoding categorical (nominal) features. One-hot encoding is a process of converting categorical data into binary vectors, where each category is represented by a unique binary vector with a 1 in the position corresponding to the category and 0s everywhere else.\n",
    "\n",
    "4. [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html): StandardScaler is a package from scikit-learn (sklearn) used for normalization. It scales the data to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "5. [sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html): train_test_split() is used to split a dataset into training and testing subsets, allowing users to evaluate the performance of machine learning models on unseen data.\n",
    "\n",
    "6. [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics): sklearn.metrics includes performance metrics functions used to evaluate a classifier's performance.\n",
    "\n",
    "7. [sklearn.tree.DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html): A decision tree classifier, limited to handling only numerical values.\n",
    "\n",
    "8. [sklearn.ensemble.RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html): A random forest classifier, limited to handling only numerical values.\n",
    "\n",
    "These packages will be utilized in following tasks for data preprocessing, classification and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ce255bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add3b97",
   "metadata": {},
   "source": [
    "First, we load the data as introduced last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993f63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('iris_modified.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befbcbc1",
   "metadata": {},
   "source": [
    "### One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb406eef",
   "metadata": {},
   "source": [
    "The scikit-learn's implementation of decision tree and random forest models does not natively handle nominal features. Instead, it requires all features to be numerical or ordinal. If your dataset contains nominal features, you'll need to preprocess them into numerical representations, such as using one-hot encoding, before fitting the models.\n",
    "\n",
    "We preprocess the nominal values using the [OneHotEncoder()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) from scikit-learn. First, we apply the [fit_transform()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder.fit_transform) method of the OneHotEncoder to the data, which both fit the encoder to the dataset and transforms it into a one-hot encoded array. We then store this resulting array as `feature_array` for further use in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9d37f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "feature_array = ohe.fit_transform(df.iloc[:,3].to_frame()).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfee9b3",
   "metadata": {},
   "source": [
    "The attribute `categories_` in the OneHotEncoder object holds the categories of each feature determined during the fitting process. To observe the newly created features, we print the `categories_` attribute, and then we store it as `feature_labels` for future reference in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e27bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['petal_width_0', 'petal_width_1', 'petal_width_2', 'petal_width_3',\n",
      "       'petal_width_4'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "print(ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79484f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = ohe.categories_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a38da",
   "metadata": {},
   "source": [
    "Then we utilize the pandas library to construct a [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) called `features`. It takes two essential components:\n",
    "\n",
    "1. `feature_array`: This is an array containing the transformed data after performing one-hot encoding on the nominal features of the original dataset.\n",
    "\n",
    "2. `columns = feature_labels`: This specifies the column names for the DataFrame. feature_labels is a list (or an array-like object) containing the names of the new features created as a result of the one-hot encoding process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61286573",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(feature_array, columns = feature_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5db70",
   "metadata": {},
   "source": [
    "When executed, this code creates a structured DataFrame where each column corresponds to one of the original nominal features, and each row represents an individual data point. In summary, the code effectively converts the transformed one-hot encoded data, along with their corresponding feature labels, into a structured DataFrame, enabling easier handling, analysis, and manipulation of the data in tabular form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d90430",
   "metadata": {},
   "source": [
    "We then merge the original numerical features with the transformed one-hot features by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bdd3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df.iloc[:,:3],features,df.iloc[:,4]],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ef920",
   "metadata": {},
   "source": [
    "We can then get some statistical information on each feature of the new dataset `df_new` by [describe()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) and [info()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6589df47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length in cm  sepal width in cm  petal length in cm  \\\n",
      "count          150.000000         150.000000          150.000000   \n",
      "mean             5.843333           3.054000            3.758667   \n",
      "std              0.828066           0.433594            1.764420   \n",
      "min              4.300000           2.000000            1.000000   \n",
      "25%              5.100000           2.800000            1.600000   \n",
      "50%              5.800000           3.000000            4.350000   \n",
      "75%              6.400000           3.300000            5.100000   \n",
      "max              7.900000           4.400000            6.900000   \n",
      "\n",
      "       petal_width_0  petal_width_1  petal_width_2  petal_width_3  \\\n",
      "count     150.000000     150.000000     150.000000     150.000000   \n",
      "mean        0.333333       0.100000       0.340000       0.206667   \n",
      "std         0.472984       0.301005       0.475296       0.406271   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         1.000000       0.000000       1.000000       0.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "       petal_width_4  \n",
      "count     150.000000  \n",
      "mean        0.020000  \n",
      "std         0.140469  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df_new.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e17f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   sepal length in cm  150 non-null    float64\n",
      " 1   sepal width in cm   150 non-null    float64\n",
      " 2   petal length in cm  150 non-null    float64\n",
      " 3   petal_width_0       150 non-null    float64\n",
      " 4   petal_width_1       150 non-null    float64\n",
      " 5   petal_width_2       150 non-null    float64\n",
      " 6   petal_width_3       150 non-null    float64\n",
      " 7   petal_width_4       150 non-null    float64\n",
      " 8   class               150 non-null    object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 10.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_new.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71920c8",
   "metadata": {},
   "source": [
    "As we can observe, all the features are numerical now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802a544",
   "metadata": {},
   "source": [
    "### Train/test split and normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d630b92",
   "metadata": {},
   "source": [
    "Last week, we briefly introduced the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. By utilizing classifiers, we can gain a deeper understanding of the data processing sequence.\n",
    "\n",
    "When working with machine learning models, it's crucial to avoid data leakage from the test set into the training process. Therefore, normalization across instances should be performed after splitting the data into training and test sets. This ensures that the normalization process relies solely on the training data, avoiding any data leakage from the test set.\n",
    "\n",
    "Furthermore, there is no need to normalize the features obtained through one-hot encoding, as they already represent binary sets with values 0 or 1.\n",
    "\n",
    "As a result, we follow this sequence:\n",
    "1. First, we use [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the dataset into training and test sets. By the default settings, it will create a test set consisting of 25% of the original data. For example, in the current dataset with 150 instances, it will lead to 38 test instances.\n",
    "\n",
    "2. After splitting the data, we then proceed with the normalization process, applying it only to the **numerical features** present in the training set. This ensures that the normalization is based on the training data and avoids any data leakage from the test set.\n",
    "\n",
    "3. Since the one-hot encoded features already represent binary sets, there is no need to perform any further normalization on them.\n",
    "\n",
    "Following this data processing sequence ensures that we have a proper setup for training and evaluating our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f80def2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_new.iloc[:,-1].values\n",
    "X_num = df_new.iloc[:,:3].values #numerical features\n",
    "X_nom = df_new.iloc[:,3:-1].values #nominal features\n",
    "X_num_train, X_num_test, X_nom_train, X_nom_test, y_train, y_test = train_test_split(X_num, X_nom, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54048d2c",
   "metadata": {},
   "source": [
    "We then apply the [StandardScaler()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to achieve the standardization (or z-score normalization). We normalize both the training and testing sets, based on the statistics calculated from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235fc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6cadd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_num_train)\n",
    "X_num_train = scaler.transform(X_num_train)\n",
    "X_num_test = scaler.transform(X_num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b73449",
   "metadata": {},
   "source": [
    "Afterward, we use [numpy.concatenate()](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html) to merge the standardized features with the one-hot encoded ones to create the complete feature set for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d505e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_num_train, X_nom_train), axis = 1)\n",
    "X_test = np.concatenate((X_num_test, X_nom_test), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d2c3b",
   "metadata": {},
   "source": [
    "You may try other normalisation method by yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bea562",
   "metadata": {},
   "source": [
    "## 2. Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45ace01",
   "metadata": {},
   "source": [
    "### 2.1 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e02c6",
   "metadata": {},
   "source": [
    "For conducting decision tree classification, we use the [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) provided by scikit-learn.\n",
    "\n",
    "By default, the parameter `criterion` is set to `'gini'`, which employs the Gini impurity as the criterion for splitting nodes in the decision tree. However, it can also be set to `'entropy'`, using information gain based on entropy.\n",
    "\n",
    "As we have already introduced information gain based on entropy in class, it might be worthy to experiment by setting the `criterion` parameter to `'entropy'` and observe the differences in the results compared to the default `'gini'` criterion. We will leave that as a self-practice.\n",
    "\n",
    "Switching the criterion allows us to assess how the decision tree's performance varies with different impurity measures and helps us gain insights into the impact of criterion selection on the model's decision boundaries and accuracy.\n",
    "\n",
    "Feel free to explore both `'gini'` and `'entropy'` criteria to better understand their effects on decision tree classification in our analysis.\n",
    "\n",
    "Note that scikit-learn's DecisionTreeClassifier does not have built-in support for splitting nodes using the `'gain ratio'` criterion. The `'gain ratio'` is an alternative to information gain that takes into account the number of categories (branches) for each attribute when making splits. While the `'gain ratio'` is not directly available in scikit-learn's DecisionTreeClassifier, you can create a custom implementation of a decision tree using the gain ratio criterion by extending the DecisionTreeClassifier class or using other libraries that offer this feature. We will leave that as an optional extension for those interested in further experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045360e",
   "metadata": {},
   "source": [
    "Concretely, We first initialize a DecisionTreeClassifier model object by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e70e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605831f",
   "metadata": {},
   "source": [
    "The [fit()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit) method is a fundamental function in scikit-learn's machine learning models used for training the model on the provided training data.\n",
    "\n",
    "We then instruct the decision tree model `dt` to learn from the provided training data. During this process, the model will analyze the features and target labels to create a decision tree that can make predictions based on the patterns and relationships it identifies in the data.\n",
    "\n",
    "After the `fit()` method completes its execution, the `dt` model will be trained and ready to make predictions on new, unseen data based on the knowledge it has acquired from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592cb214",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942cbd62",
   "metadata": {},
   "source": [
    "Then, we classify the test data by [predict()](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c4d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e591972",
   "metadata": {},
   "source": [
    "Using the method introduces last week, we can evaluate the test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ad359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of decision tree on the dataset is:  0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "acc_dt = metrics.accuracy_score(y_test, y_pred_dt)\n",
    "print(\"The test accuracy of decision tree on the dataset is: \", acc_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f380a835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test macro f1-score of decision tree on the dataset is:  0.9444444444444445\n"
     ]
    }
   ],
   "source": [
    "f1_dt = metrics.f1_score(y_test, y_pred_dt, average='macro')\n",
    "print(\"The test macro f1-score of decision tree on the dataset is: \", f1_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984abd62",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643eb75",
   "metadata": {},
   "source": [
    "Similar to the process described above, we utilize the [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) provided in scikit-learn for our classification tasks.\n",
    "\n",
    "The `RandomForestClassifier` is an ensemble learning method that constructs multiple decision trees and combines their predictions to improve overall accuracy and reduce overfitting. By default, the parameter `criterion` in the `RandomForestClassifier` is set to `'gini'`, which uses the Gini impurity as the criterion for splitting nodes in the individual decision trees.\n",
    "\n",
    "Feel free to explore both `'gini'` and `'entropy'` criteria in your random forest classification to further enhance your comprehension of their effects on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48f39a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=0) \n",
    "rf = rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09ee7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3916405",
   "metadata": {},
   "source": [
    "Then, we do test by [predict()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6787a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f33cb6e",
   "metadata": {},
   "source": [
    "And we evaluate by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa44aa0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy of random forest on the dataset is:  0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "acc_rf = metrics.accuracy_score(y_test, y_pred_rf)\n",
    "print(\"The test accuracy of random forest on the dataset is: \", acc_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b414e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test macro f1-score of random forest on the dataset is:  0.9717034521788342\n"
     ]
    }
   ],
   "source": [
    "f1_rf = metrics.f1_score(y_test, y_pred_rf, average='macro')\n",
    "print(\"The test macro f1-score of random forest on the dataset is: \", f1_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd8920",
   "metadata": {},
   "source": [
    "We can see the performance of random tree is better than decision tree on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e9428",
   "metadata": {},
   "source": [
    "Author: *Kaki Zhou* 3/8/2023 "
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Kaki Zhou"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
